---
title: "Unsupervised Learning: Clustering Algorithms, Distance Metrics & Evaluation"
description: "A comprehensive guide covering clustering basics, K-Means, hierarchical methods, distance functions, data standardization, and cluster evaluation techniques."
pubDate: "Nov 15 2023"
tags: ["Machine Learning", "Cluster", "K-Means", "Hierarchical Clustering", "Distance Metrics", "Data Standardization", "Cluster Evaluation"]
---

## Unsupervised Learning

### 1. Basic Concepts

**Clustering** is often referred to as unsupervised learning. It is a technique for discovering similar groups within data, addressing the problem of prediction for training samples with unknown labels. Historically, clustering and unsupervised learning have been closely linked, leading to the widespread perception that they are the same concept. In reality, association rule mining is also a form of unsupervised learning.

Clustering is the process of grouping a collection of physical or abstract objects into classes of similar objects.

The groups generated by clustering are called **clusters**. A cluster is a collection of data objects where:

- Objects within the same cluster have high similarity to each other.
- Objects in different clusters have high dissimilarity.

Clustering can serve as a standalone process to uncover the intrinsic distribution structure of data, or as a preprocessing step for other learning tasks such as classification.

Based on different learning strategies, major clustering algorithms can generally be categorized into the following five types:

- Partitioning methods
- Hierarchical methods
- Density-based methods
- Grid-based methods
- Model-based methods

The quality of a clustering method is generally evaluated based on **intra-class difference** (cohesion) and **inter-class difference** (separation). The quality and representation of clustering results depend heavily on the algorithm, distance function, and application domain.

----

### 2. K-Means

The K-Means algorithm is a **partitioning-based clustering algorithm**. It iteratively assigns data points into $K$ clusters based on a distance function, where $K$ is a user-specified parameter.

> **Partitioning-based clustering algorithms**: These algorithms adopt a strategy of minimizing an objective function, iteratively partitioning data objects into $k$ groups, where each group forms a cluster.

**Steps of the K-Means algorithm:**

1.  Randomly select $K$ data points as **initial cluster centers** (centroids).
2.  Calculate the distance between each data point and each center, assigning each point to the nearest cluster center.
3.  After assignment, recalculate the cluster center based on the data points within each cluster.
4.  Repeat the above process until **the maximum number of iterations is reached**, **the shift in recalculated cluster centers is less than a specified threshold**, **the assignment of data points no longer changes**, or **the Sum of Squared Errors (SSE) reaches a local minimum**.

> **Sum of Squared Error (SSE)**:
> $$
> SSE=\sum\limits^k_{j=1}\sum\limits_{x\in Cluster_j}distance(x,m_j)^2
> $$
> Where $Cluster_j$ represents the $j$-th cluster, and $m_j$ is its center.

**Advantages of K-Means:**

-   Easy to understand and implement.
-   Efficient: The time complexity is $O(tkn)$, where $t$ is the number of iterations, $k$ is the number of clusters, and $n$ is the number of data points.

**Disadvantages of K-Means:**

-   Applicable only when the mean of the data can be defined.
-   Requires specifying the number of clusters $k$ in advance.
-   Sensitive to noise and outliers.
-   Sensitive to the initialization of cluster centers.
-   Not suitable for discovering clusters with non-convex shapes (e.g., shapes that are not hyper-ellipsoids or spheres).

**Methods for Handling Outliers:**

Remove outliers during the clustering process, or use random sampling methodsâ€”perform pre-clustering on a subset of data points, and then assign all data points to these clusters.

----

### 3. Cluster Representation

**Cluster Center Representation**: Represents each cluster using its center (centroid). The radius and standard deviation of the cluster are calculated to determine its spread in each dimension. This is suitable for clusters with high-dimensional spherical shapes.

**Classification Model Representation**: Treats sample points within the same cluster as belonging to the same class, and applies other classification methods to discover a classification model based on the clustering results.

**Frequent Value Representation**: Represents the cluster using the most frequent values within it.

----

### 4. Hierarchical Clustering

Hierarchical clustering attempts to partition the dataset at different levels, forming a tree-like clustering structure. There are two approaches: **Bottom-Up** (Agglomerative) and **Top-Down** (Divisive).

**Bottom-Up (Agglomerative)**: Starts building the cluster tree from the bottom. Each sample is initially treated as a separate cluster. The two most similar clusters are iteratively merged to form clusters at the next level, until all data points are in a single cluster (Root).

**Top-Down (Divisive)**: Starts with a single cluster containing all data points and recursively splits it until each cluster contains only one data point.

Agglomerative clustering (Bottom-Up) is more widely used.

**Calculating Distance Between Two Clusters:**

*   **Single Linkage**: The distance between two clusters is the distance between the two closest data points in the clusters. This method is good for finding arbitrarily shaped clusters but is sensitive to noise (outliers).
*   **Complete Linkage**: The distance between two clusters is the maximum distance between any two data points in the clusters. It is also sensitive to noise.
*   **Average Linkage**: The distance between two clusters is the average of the distances between all pairs of data points in the two clusters.
*   **Centroid Method**: The distance between two clusters is the distance between their centroids.

The computational complexity of hierarchical clustering algorithms is at least $O(n^2)$, making them inefficient for large-scale datasets.

----

### 5. Distance Functions

#### 5.1 Distance Between Numerical Attributes

The most commonly used distance functions are **Euclidean Distance** and **Manhattan Distance**, both of which are special cases of the **Minkowski Distance**.

> **Minkowski Distance**
> $$
> Minkowski\_Distance(x_i,x_j)=(\sum\limits^{d}_{k=1}|x_{ik}-x_{jk}|^p)^{\frac{1}{p}}
> $$
> $p$ is usually an integer. When $p=1$, it represents the Manhattan Distance; when $p=2$, it represents the Euclidean Distance.

**Weighted Euclidean Distance:**
$$
dist(x_i,x_j)=\sqrt{\omega_1(x_{i1}-x_{j1})^2+\omega_2(x_{i2}-x_{j2})^2+...\omega_d(x_{id}-x_{jd})^2}
$$

**Squared Euclidean Distance:** Increases the weight of data points that are further apart.
$$
dist(x_i,x_j)=(x_{i1}-x_{j1})^2+(x_{i2}-x_{j2})^2+...(x_{id}-x_{jd})^2
$$

**Chebyshev Distance:**
$$
dist(x_i,x_j)=max(|x_{i1}-x_{j1}|,|x_{i2}-x_{j2}|,...|x_{id}-x_{jd}|)
$$

#### 5.2 Distance Between Boolean Attributes

Confusion Matrix:

![51316be3faa77c5bea2abca95e0aaa65.png](https://s2.loli.net/2023/11/15/AfI9COuPgEMQ5ph.png)

When the two states of a Boolean attribute are equally important and carry the same weight, the attribute is called **symmetric**. The most common distance function for symmetric attributes is the **Simple Matching Coefficient**, which is the proportion of attributes that do not match:
$$
dist(x_i,x_j)=\frac{b+c}{a+b+c+d}
$$

When one state of a Boolean attribute is more important than the other, the attribute is called **asymmetric**. Typically, 1 represents the more important state, which is usually the less frequent one. The most common distance metric for asymmetric attributes is the **Jaccard Distance** (Jaccard Coefficient):
$$
dist(x_i,x_j)=\frac{b+c}{a+b+c}
$$

#### 5.3 Distance Between Categorical (Symbolic) Attributes

Similar to Boolean attributes, the distance metric for categorical attributes is also based on the simple matching distance:
$$
dist(x_i,x_j)=\frac{d-q}{d}
$$
Where $d$ is the total number of attributes, and $q$ is the number of matching attribute values.

#### 5.4 Distance Between Text Documents

Text documents consist of sequences of words forming sentences. Therefore, documents can be represented as vectors, just like ordinary data points. When comparing two documents, similarity is usually calculated instead of distance. The most widely used similarity function is **Cosine Similarity**:
$$
cos(\theta)=\frac{\sum\limits^d_{i=1}(x_i\times y_i)}{\sqrt{\sum\limits^d_{i=1}(x_i)^2}\times\sqrt{\sum\limits^d_{i=1}(y_i^2)}}
$$

----

### 6. Data Standardization

When using Euclidean distance, data standardization is performed to ensure that all attributes contribute equally to the distance calculation.

**Standardizing Attributes**: Forces all attributes to vary within the same range.

#### 6.1 Interval-Scaled Attributes

Refers to real values on a linear scale. Two main standardization methods are **Min-Max Normalization (Range Standardization)** and **Z-score Normalization**.

**Min-Max Normalization:**
$$
range(x_i)=\frac{x_i-min_f(x_if)}{max_f(x_if)-min_f(x_if)}
$$

**Z-score Normalization:** Standardizes attributes based on their mean and standard deviation. The Z-score value indicates the direction and distance of an attribute value from the mean. The calculation formula is as follows:
$$
m_f=\frac 1 n (x_{1f}+x_{2f}+...+x_{nf})\\
s_f=\frac 1 n (|x_{1f}-m_f|+|x_{2f}-m_f|+...+|x_{nf}-m_f|)\\
z\_score(x_{if})=\frac {x_{if}-m_f} {s_f}
$$

#### 6.2 Ratio-Scaled Attributes

Non-linear numerical attributes. These can be transformed using a logarithmic function and then standardized as interval-scaled attributes.

#### 6.3 Categorical (Symbolic) Attributes

Convert categorical attributes to Boolean attributes:

-   If a categorical attribute has $v$ possible values.
-   Create $v$ Boolean attributes to represent them.
-   When a data instance takes a specific value for that categorical attribute, set the corresponding Boolean attribute to 1 and the others to 0.

Essentially, this is **One-hot Encoding**.

#### 6.4 Ordinal Attributes

A common standardization method is to treat ordinal attributes as interval-scaled attributes for standardization.

#### 6.5 Handling Mixed Attributes

**Convert to a Single Type**: First determine a dominant attribute type among the mixed attributes, then convert other types of attributes in the dataset to that type.

**Combined Distance**: First calculate the distance between two data points separately for each different type of attribute, then combine these distances to obtain a final distance.

----

### 7. Selection of Clustering Algorithms

Execute different algorithms, use different distance functions, and apply different parameter settings, then compare the final results.

Understanding the results requires both a deep understanding of the raw data and knowledge of the algorithms used.

----

### 8. Cluster Evaluation

#### 8.1 Cluster Evaluation Based on External Information

Use a labeled dataset (classification dataset) to evaluate the clustering algorithm.

**Entropy**: For each cluster, its entropy can be calculated as follows:
$$
entropy(D_i)=-\sum\limits^k_{j=1}Pr_i(c_j)log_2Pr_i(c_j)
$$
Where $Pr_i(c_j)$ is the proportion of data points in cluster $i$ (or $D_i$) that belong to class $c_j$. The total entropy of the clustering result is:
$$
entropy(D)=\sum\limits^k_{i=1}\frac{|D_i|}{|D|}\times entropy(D_i)
$$

**Purity**: Measures the extent to which a cluster contains data from only one class.
$$
purity(D_i)=max_j(Pr_i(c_j))
$$
The total purity of the clustering result is:
$$
purity(D)=\sum\limits^k_{i=1}\frac{|D_i|}{|D|}\times purity(D_i)
$$

#### 8.2 Cluster Evaluation Based on Internal Information

**Intra-cluster Cohesion**: Similarity between data points and the cluster center. Typically measured using SSE (Sum of Squared Errors).

**Inter-cluster Separation**: The degree of difference between different cluster centers.
